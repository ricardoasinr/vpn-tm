# Datawarehouse - Moreno Baldivieso

Sistema de extracciÃ³n de datos para el datawarehouse de Moreno Baldivieso. Este proyecto permite extraer datos desde dos fuentes principales: una base de datos Aurora MySQL y una API GraphQL, guardando los resultados en archivos CSV.

## ğŸ“‹ DescripciÃ³n

Este proyecto proporciona herramientas para extraer datos de diferentes fuentes y prepararlos para su uso en un datawarehouse. Incluye tres mÃ©todos principales de extracciÃ³n:

1. **ExtracciÃ³n desde Base de Datos**: Ejecuta consultas SQL directamente en una base de datos Aurora MySQL
2. **ExtracciÃ³n desde API (AutomÃ¡tica)**: Realiza peticiones GraphQL a una API REST y procesa las respuestas automÃ¡ticamente
3. **ExtracciÃ³n desde API (Interactiva)**: Ejecuta comandos curl almacenados en archivos con soporte para paginaciÃ³n automÃ¡tica mediante menÃº interactivo

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.7 o superior
- Acceso a la base de datos Aurora MySQL (puede requerir VPN)
- Credenciales de acceso a la API

### Pasos de instalaciÃ³n

1. Clonar o descargar el repositorio
2. Instalar las dependencias:

```bash
pip install -r requirements.txt
```

Las dependencias incluyen:
- `pymysql>=1.0.0` - Para conexiÃ³n a MySQL
- `requests>=2.31.0` - Para peticiones HTTP

3. (Opcional) Configurar variables de entorno:
   - Copia `.env.example` a `.env` y configura tus credenciales
   - O modifica directamente `config/database.py` y `config/api.py`

## ğŸ“ Estructura del Proyecto

```
datawarehouse-mb/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/              # Extractores principales
â”‚   â”‚   â”œâ”€â”€ database_extractor.py      # Extrae desde MySQL
â”‚   â”‚   â”œâ”€â”€ api_extractor.py           # ExtracciÃ³n automÃ¡tica de API GraphQL
â”‚   â”‚   â””â”€â”€ interactive_api_extractor.py  # ExtracciÃ³n interactiva de API GraphQL
â”‚   â”œâ”€â”€ api/                     # MÃ³dulos de API
â”‚   â”‚   â”œâ”€â”€ auth.py              # AutenticaciÃ³n
â”‚   â”‚   â”œâ”€â”€ client.py            # Cliente GraphQL
â”‚   â”‚   â””â”€â”€ pagination.py        # LÃ³gica de paginaciÃ³n
â”‚   â”œâ”€â”€ database/                # MÃ³dulos de base de datos
â”‚   â”‚   â””â”€â”€ connection.py        # ConexiÃ³n a MySQL
â”‚   â””â”€â”€ utils/                   # Utilidades
â”‚       â”œâ”€â”€ csv_writer.py         # Escritura de CSV
â”‚       â”œâ”€â”€ json_flattener.py    # Aplanar JSON anidado
â”‚       â”œâ”€â”€ curl_parser.py       # Parsear comandos curl
â”‚       â””â”€â”€ graphql_parser.py    # Parsear respuestas GraphQL
â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ sql/                     # Consultas SQL
â”‚   â”‚   â”œâ”€â”€ dimensions/
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_asuntos.sql
â”‚   â”‚   â”‚   â””â”€â”€ dim_usuarios.sql
â”‚   â”‚   â””â”€â”€ facts/
â”‚   â”‚       â”œâ”€â”€ hechos_tiempos.sql
â”‚   â”‚       â””â”€â”€ hechos_capacidad.sql
â”‚   â””â”€â”€ graphql/                 # Queries GraphQL
â”‚       â”œâ”€â”€ dim_asuntos.graphql
â”‚       â”œâ”€â”€ dim_asuntos.variables.json
â”‚       â”œâ”€â”€ dim_usuarios.graphql
â”‚       â”œâ”€â”€ dim_usuarios.variables.json
â”‚       â”œâ”€â”€ hechos_tiempos.graphql
â”‚       â””â”€â”€ hechos_tiempos.variables.json
â”œâ”€â”€ config/                      # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ database.py              # Config BD
â”‚   â””â”€â”€ api.py                   # Config API
â”œâ”€â”€ output/                      # Resultados
â”‚   â”œâ”€â”€ database/                # Resultados de extracciones desde BD
â”‚   â””â”€â”€ api/                     # Resultados de extracciones desde API
â”œâ”€â”€ resources/                   # Recursos externos
â”‚   â”œâ”€â”€ postman/
â”‚   â”‚   â””â”€â”€ emba.postman_collection.json
â”‚   â””â”€â”€ vpn/
â”‚       â””â”€â”€ mb.ovpn
â”œâ”€â”€ scripts/                     # Scripts de ejecuciÃ³n
â”‚   â”œâ”€â”€ extract_all.py           # Ejecuta todos los extractores
â”‚   â”œâ”€â”€ extract_dim_asuntos.py   # Extrae solo Dim_Asuntos (API)
â”‚   â”œâ”€â”€ extract_dim_usuarios.py  # Extrae solo Dim_Usuarios (API)
â”‚   â””â”€â”€ extract_hechos_tiempos.py # Extrae solo Hechos_Tiempos (API)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Uso

### Scripts de EjecuciÃ³n RÃ¡pida

El proyecto incluye scripts convenientes en la carpeta `scripts/` para ejecutar extracciones de forma rÃ¡pida:

#### Ejecutar Todos los Extractores

Ejecuta todas las extracciones (base de datos y API) en secuencia:

```bash
python scripts/extract_all.py
```

#### Ejecutar Extractores Individuales de API

Puedes ejecutar cada query GraphQL de forma individual:

**Dim_Asuntos:**
```bash
python scripts/extract_dim_asuntos.py
```

**Dim_Usuarios:**
```bash
python scripts/extract_dim_usuarios.py
```

**Hechos_Tiempos:**
```bash
python scripts/extract_hechos_tiempos.py
```

**CaracterÃ­sticas de los scripts individuales:**
- AutenticaciÃ³n automÃ¡tica
- PaginaciÃ³n automÃ¡tica
- Guarda resultados en `output/api/`
- Mensajes de progreso claros
- Manejo de errores robusto

### 1. ExtracciÃ³n desde Base de Datos

Ejecuta todas las consultas SQL en `queries/sql/` y guarda los resultados en CSV.

```bash
python -m src.extractors.database_extractor
```

O desde la raÃ­z del proyecto:

```bash
python src/extractors/database_extractor.py
```

**CaracterÃ­sticas:**
- Conecta directamente a la base de datos Aurora MySQL
- Ejecuta todos los archivos `.sql` en `queries/sql/` (recursivo)
- Guarda los resultados en `output/database/`
- Soporta mÃºltiples consultas en batch

**ConfiguraciÃ³n:**
Las credenciales estÃ¡n en `config/database.py`. Puedes usar variables de entorno:
- `DB_HOST` - Host de la base de datos
- `DB_PORT` - Puerto (default: 3306)
- `DB_NAME` - Nombre de la base de datos
- `DB_USER` - Usuario
- `DB_PASSWORD` - ContraseÃ±a

### 2. ExtracciÃ³n desde API (AutomÃ¡tica)

Realiza peticiones GraphQL a la API y guarda los resultados en CSV automÃ¡ticamente. Ejecuta **todos** los queries GraphQL encontrados en `queries/graphql/`.

```bash
python -m src.extractors.api_extractor
```

O desde la raÃ­z del proyecto:

```bash
python src/extractors/api_extractor.py
```

**CaracterÃ­sticas:**
- AutenticaciÃ³n automÃ¡tica mediante login
- Soporte para paginaciÃ³n automÃ¡tica
- Extrae datos de todos los queries GraphQL en `queries/graphql/`:
  - `Dim_Asuntos` (BusinessMeta)
  - `Dim_Usuarios` (Users)
  - `Hechos_Tiempos` (TimesByFiltersPaged)
- Guarda resultados en `output/api/`
- Convierte respuestas JSON anidadas a CSV plano

**ConfiguraciÃ³n:**
Las credenciales estÃ¡n en `config/api.py`. Puedes usar variables de entorno:
- `API_BASE_URL` - URL base de la API
- `API_USERNAME` - Usuario para login
- `API_PASSWORD` - ContraseÃ±a para login
- `API_TENANT_NAME` - Nombre del tenant

### 3. ExtracciÃ³n desde API (Interactiva)

Script interactivo que permite ejecutar comandos curl almacenados en archivos.

```bash
python -m src.extractors.interactive_api_extractor
```

O desde la raÃ­z del proyecto:

```bash
python src/extractors/interactive_api_extractor.py
```

**CaracterÃ­sticas:**
- MenÃº interactivo para seleccionar quÃ© query ejecutar
- Lee queries GraphQL desde archivos `.graphql` y variables desde `.variables.json`
- Soporte para paginaciÃ³n automÃ¡tica en queries GraphQL
- Guarda resultados en formato JSON y CSV en `output/api/`
- Permite ejecutar mÃºltiples queries en la misma sesiÃ³n

## ğŸ“Š Datos ExtraÃ­dos

### Dimensiones
- **Dim_Asuntos**: InformaciÃ³n de asuntos/negocios (BusinessMeta)
- **Dim_Usuarios**: InformaciÃ³n de usuarios del sistema

### Hechos
- **Hechos_Tiempos**: Registros de tiempos trabajados
- **Hechos_Capacidad**: Datos de capacidad (solo desde BD)

## ğŸ” Seguridad

âš ï¸ **Importante**: Este proyecto contiene credenciales hardcodeadas por defecto. Para uso en producciÃ³n:

1. Usa variables de entorno para credenciales (ver `config/database.py` y `config/api.py`)
2. No subas archivos con credenciales a repositorios pÃºblicos
3. Considera usar un gestor de secretos (AWS Secrets Manager, etc.)
4. Crea un archivo `.env` (no incluido en el repo) con tus credenciales

## ğŸ› ï¸ Funcionalidades Adicionales

### Listar tablas de la base de datos

```python
from src.database.connection import list_database_tables
list_database_tables()
```

## ğŸ“ Notas

- Los scripts manejan automÃ¡ticamente la paginaciÃ³n en las respuestas de la API
- Los datos anidados de JSON se aplanan automÃ¡ticamente al convertir a CSV
- Los archivos CSV se guardan con codificaciÃ³n UTF-8
- Los scripts crean automÃ¡ticamente las carpetas de resultados si no existen
- Los nombres de archivos usan `snake_case` para consistencia
- Los scripts en `scripts/` son ejecutables directamente y proporcionan una forma conveniente de ejecutar extracciones especÃ­ficas
- Usa `extract_all.py` para ejecutar todas las extracciones en una sola ejecuciÃ³n

## ğŸ¤ Contribuciones

Para agregar nuevas extracciones:

1. **Para consultas SQL**: Agrega un archivo `.sql` en `queries/sql/dimensions/` o `queries/sql/facts/`
   - El extractor de base de datos los ejecutarÃ¡ automÃ¡ticamente

2. **Para queries GraphQL automÃ¡ticos**: 
   - Crea un archivo `.graphql` con el query en `queries/graphql/`
   - Crea un archivo `.variables.json` con las variables (opcional, puede estar vacÃ­o `{}`)
   - Ejemplo: `dim_nuevo.graphql` y `dim_nuevo.variables.json`
   - El extractor automÃ¡tico (`api_extractor.py`) los ejecutarÃ¡ automÃ¡ticamente
   - Opcionalmente, crea un script individual en `scripts/extract_nuevo.py` siguiendo el patrÃ³n de los existentes

3. **Para queries GraphQL interactivos**: 
   - Los mismos archivos `.graphql` y `.variables.json` funcionan con el extractor interactivo
   - El extractor interactivo mostrarÃ¡ todos los queries disponibles en un menÃº

## ğŸ“„ Licencia

Este proyecto es de uso interno de Moreno Baldivieso.
