# Datawarehouse - Moreno Baldivieso

Sistema de extracciÃ³n de datos para el datawarehouse de Moreno Baldivieso. Este proyecto permite extraer datos desde dos fuentes principales: una base de datos Aurora MySQL y una API GraphQL, guardando los resultados en archivos CSV.

## ğŸ“‹ DescripciÃ³n

Este proyecto proporciona herramientas para extraer datos de diferentes fuentes y prepararlos para su uso en un datawarehouse. Incluye tres mÃ©todos principales de extracciÃ³n:

1. **ExtracciÃ³n desde Base de Datos**: Ejecuta consultas SQL directamente en una base de datos Aurora MySQL
2. **ExtracciÃ³n desde API**: Realiza peticiones GraphQL a una API REST y procesa las respuestas
3. **ExtracciÃ³n mediante CURL**: Ejecuta comandos curl almacenados en archivos de texto con soporte para paginaciÃ³n automÃ¡tica

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.7 o superior
- Acceso a la base de datos Aurora MySQL (puede requerir VPN)
- Credenciales de acceso a la API

### Pasos de instalaciÃ³n

1. Clonar o descargar el repositorio
2. Instalar las dependencias:

```bash
pip install -r requirements.txt
```

Las dependencias incluyen:
- `pymysql>=1.0.0` - Para conexiÃ³n a MySQL
- `requests>=2.31.0` - Para peticiones HTTP

## ğŸ“ Estructura del Proyecto

```
Datawarehouse/
â”œâ”€â”€ api_requests.py          # Script para extraer datos desde la API GraphQL
â”œâ”€â”€ bd.py                    # Script para extraer datos desde la base de datos
â”œâ”€â”€ run_curl.py              # Script interactivo para ejecutar comandos curl
â”œâ”€â”€ requirements.txt         # Dependencias de Python
â”œâ”€â”€ curls/                   # Archivos con comandos curl
â”‚   â”œâ”€â”€ Dim_Asuntos.txt
â”‚   â”œâ”€â”€ Dim_Usuarios.txt
â”‚   â”œâ”€â”€ Hechos_Tiempos.txt
â”‚   â””â”€â”€ login.txt
â”œâ”€â”€ queries/                 # Consultas SQL
â”‚   â”œâ”€â”€ Dim_Asuntos.sql
â”‚   â”œâ”€â”€ Dim_Usuario.sql
â”‚   â”œâ”€â”€ Hechos_Capacidad.sql
â”‚   â””â”€â”€ Hechos_Tiempos.sql
â”œâ”€â”€ results_api/             # Resultados de extracciones desde API
â”‚   â”œâ”€â”€ Dim_Asuntos.csv
â”‚   â””â”€â”€ Dim_Usuario.csv
â”œâ”€â”€ results_bd/              # Resultados de extracciones desde BD
â”‚   â”œâ”€â”€ Dim_Asuntos.csv
â”‚   â”œâ”€â”€ Dim_Usuario.csv
â”‚   â”œâ”€â”€ Hechos_Capacidad.csv
â”‚   â””â”€â”€ Hechos_Tiempos.csv
â””â”€â”€ extras/                  # Archivos adicionales
    â”œâ”€â”€ emba.postman_collection.json
    â””â”€â”€ mb.ovpn
```

## ğŸ”§ Uso

### 1. ExtracciÃ³n desde Base de Datos (`bd.py`)

Ejecuta todas las consultas SQL en la carpeta `queries/` y guarda los resultados en CSV.

```bash
python bd.py
```

**CaracterÃ­sticas:**
- Conecta directamente a la base de datos Aurora MySQL (sin VPN)
- Ejecuta todos los archivos `.sql` en la carpeta `queries/`
- Guarda los resultados en `results_bd/` (o `results/` segÃºn configuraciÃ³n)
- Soporta mÃºltiples consultas en batch

**ConfiguraciÃ³n:**
Las credenciales de la base de datos estÃ¡n configuradas en `bd.py`. Puedes usar variables de entorno:
- `DB_HOST` - Host de la base de datos
- `DB_PORT` - Puerto (default: 3306)
- `DB_NAME` - Nombre de la base de datos
- `DB_USER` - Usuario
- `DB_PASSWORD` - ContraseÃ±a

### 2. ExtracciÃ³n desde API (`api_requests.py`)

Realiza peticiones GraphQL a la API y guarda los resultados en CSV.

```bash
python api_requests.py
```

**CaracterÃ­sticas:**
- AutenticaciÃ³n automÃ¡tica mediante login
- Soporte para paginaciÃ³n automÃ¡tica
- Extrae datos de:
  - `Dim_Asuntos` (BusinessMeta)
  - `Dim_Usuario` (Users)
  - `Hechos_Tiempos` (TimesByFiltersPaged)
- Guarda resultados en `results_api/`
- Convierte respuestas JSON anidadas a CSV plano

**Nota:** Las credenciales de login estÃ¡n hardcodeadas en el script. Considera usar variables de entorno para mayor seguridad.

### 3. ExtracciÃ³n mediante CURL (`run_curl.py`)

Script interactivo que permite ejecutar comandos curl almacenados en archivos.

```bash
python run_curl.py
```

**CaracterÃ­sticas:**
- MenÃº interactivo para seleccionar quÃ© curl ejecutar
- Soporte para paginaciÃ³n automÃ¡tica en queries GraphQL
- Guarda resultados en formato JSON y CSV en `results_curl/`
- Permite ejecutar mÃºltiples curls en la misma sesiÃ³n

## ğŸ“Š Datos ExtraÃ­dos

### Dimensiones
- **Dim_Asuntos**: InformaciÃ³n de asuntos/negocios (BusinessMeta)
- **Dim_Usuario**: InformaciÃ³n de usuarios del sistema

### Hechos
- **Hechos_Tiempos**: Registros de tiempos trabajados
- **Hechos_Capacidad**: Datos de capacidad (solo desde BD)

## ğŸ” Seguridad

âš ï¸ **Importante**: Este proyecto contiene credenciales hardcodeadas. Para uso en producciÃ³n:

1. Usa variables de entorno para credenciales
2. No subas archivos con credenciales a repositorios pÃºblicos
3. Considera usar un gestor de secretos (AWS Secrets Manager, etc.)

## ğŸ› ï¸ Funcionalidades Adicionales

### Listar tablas de la base de datos

Puedes modificar `bd.py` para usar la funciÃ³n `list_database_tables()` que lista todas las tablas disponibles en la base de datos.

## ğŸ“ Notas

- Los scripts manejan automÃ¡ticamente la paginaciÃ³n en las respuestas de la API
- Los datos anidados de JSON se aplanan automÃ¡ticamente al convertir a CSV
- Los archivos CSV se guardan con codificaciÃ³n UTF-8
- Los scripts crean automÃ¡ticamente las carpetas de resultados si no existen

## ğŸ¤ Contribuciones

Para agregar nuevas extracciones:

1. **Para consultas SQL**: Agrega un archivo `.sql` en `queries/`
2. **Para queries GraphQL**: Modifica `api_requests.py` o agrega un archivo `.txt` en `curls/`

## ğŸ“„ Licencia

Este proyecto es de uso interno de Moreno Baldivieso.

